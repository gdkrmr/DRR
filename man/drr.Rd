% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DRR.R
\name{drr}
\alias{drr}
\title{Dimensionality Reduction via Regression}
\usage{
drr(X, ndim = ncol(X), lambda = c(0, 10^(-3:2)), kernel = "rbfdot",
  kernel.pars = list(sigma = 10^(-3:4)), pca = TRUE, pca.center = TRUE,
  pca.scale = FALSE, fastcv = FALSE, cv.folds = 5, fastcv.test = NULL,
  fastkrr.nblocks = 4, verbose = TRUE)
}
\arguments{
\item{X}{input data, a matrix.}

\item{ndim}{the number of output dimensions and regression
functions to be estimated, see details for inversion.}

\item{lambda}{the penalty term for the Kernel Ridge Regression.}

\item{kernel}{a kernel function or string, see
\code{\link[kernlab]{kernel-class}} for details.}

\item{kernel.pars}{a list with parameters for the kernel. each
parameter can be a vector, crossvalidation will choose the best
combination.}

\item{pca}{logical, do a preprocessing using pca.}

\item{pca.center}{logical, center data before applying pca.}

\item{pca.scale}{logical, scale data before applying pca.}

\item{fastcv}{if \code{TRUE} uses \code{\link[CVST]{fastCV}}, if
\code{FALSE} uses \code{\link[CVST]{CV}} for crossvalidation.}

\item{cv.folds}{if using normal crossvalidation, the number of
folds to be used.}

\item{fastcv.test}{an optional separate test data set to be used
for \code{\link[CVST]{fastCV}}, handed over as option
\code{test} to \code{\link[CVST]{fastCV}}.}

\item{fastkrr.nblocks}{the number of blocks used for fast KRR,
higher numbers are faster to compute but may introduce
numerical inaccurracies, see
\code{\link{constructFastKRRLearner}} for details.}

\item{verbose}{logical, should the crossvalidation report back.}
}
\value{
A list the following items:
\itemize{
 \item {"fitted.data"} The data in reduced dimensions.
 \item {"pca.means"} The means used to center the original data.
 \item {"pca.scale"} The standard deviations used to scale the original data.
 \item {"pca.rotation"} The rotation matrix of the PCA.
 \item {"models"} A list of models used to estimate each dimension.
 \item {"apply"} A function to fit new data to the estimated model.
 \item {"inverse"} A function to untransform data. 
}
}
\description{
\code{drr} Implements Dimensionality Reduction via Regression using
Kernel Ridge Regression.
}
\details{
Parameter combination will be formed and cross-validation used to
select the best combination. Cross-validation uses
\code{\link[CVST]{CV}} or \code{\link[CVST]{fastCV}}.

Pre-treatment of the data using a PCA and scaling is made
\eqn{\alpha = Vx}.  the representation in reduced dimensions is

\deqn{y_i = \alpha - f_i(\alpha_1, \ldots, \alpha_{i-1})}

then the final DRR representation is:

\deqn{r = (\alpha_1, y_2, y_3, \ldots,y_d)}

DRR is invertible by

\deqn{\alpha_i = y_i + f_i(\alpha_1,\alpha_2, \ldots, alpha_{i-1})}

If less dimensions are estimated, there will be less inverse
functions and calculating the inverse will be inaccurate.
}
\examples{
tt <- seq(0,4*pi, length.out = 200)
helix <- cbind(
  x = 3 * cos(tt) + rnorm(length(tt), sd = seq(0.1, 1.4, length.out = length(tt))),
  y = 3 * sin(tt) + rnorm(length(tt), sd = seq(0.1, 1.4, length.out = length(tt))),
  z = 2 * tt      + rnorm(length(tt), sd = seq(0.1, 1.4, length.out = length(tt)))
)
helix <- helix[sample(nrow(helix)),] # shuffling data is important!!
system.time(
drr.fit  <- drr(helix, ndim = 3, cv.folds = 4,
                lambda = 10^(-2:1),
                kernel.pars = list(sigma = 10^(0:3)),
                fastkrr.nblocks = 2, verbose = TRUE,
                fastcv = FALSE)
)

\dontrun{
library(rgl)
plot3d(helix)
points3d(drr.fit$inverse(drr.fit$fitted.data[,1,drop = FALSE]), col = 'blue')
points3d(drr.fit$inverse(drr.fit$fitted.data[,1:2]),             col = 'red')

plot3d(drr.fit$fitted.data)
pad <- -3
fd <- drr.fit$fitted.data
xx <- seq(min(fd[,1])      , max(fd[,1])      , length.out = 25)
yy <- seq(min(fd[,2]) - pad, max(fd[,2]) + pad, length.out = 5)
zz <- seq(min(fd[,3]) - pad, max(fd[,3]) + pad, length.out = 5)

dd <- as.matrix(expand.grid(xx, yy, zz))
plot3d(helix)
for(y in yy) for(x in xx)
  rgl.linestrips(drr.fit$inverse(cbind(x, y, zz)), col = 'blue')
for(y in yy) for(z in zz)
  rgl.linestrips(drr.fit$inverse(cbind(xx, y, z)), col = 'blue')
for(x in xx) for(z in zz)
  rgl.linestrips(drr.fit$inverse(cbind(x, yy, z)), col = 'blue')
}

}
\references{
Laparra, V., Malo, J., Camps-Valls, G., 2015. Dimensionality
    Reduction via Regression in Hyperspectral Imagery. IEEE Journal
    of Selected Topics in Signal Processing 9,
    1026-1036. doi:10.1109/JSTSP.2015.2417833
}

