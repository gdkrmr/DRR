[{"path":"/articles/comparePCA.html","id":"load-libraries","dir":"Articles","previous_headings":"","what":"Load libraries","title":"Comparing DRR and PCA","text":"","code":"library(DRR) set.seed(123)"},{"path":"/articles/comparePCA.html","id":"read-in-data","dir":"Articles","previous_headings":"","what":"Read in data","title":"Comparing DRR and PCA","text":"","code":"data(iris)  in_data <- iris[, 1:4]  npoints <- nrow(in_data) nvars <- ncol(in_data) for (i in seq_len(nvars)) in_data[[i]] <- as.numeric(in_data[[i]]) my_data <- scale(in_data[sample(npoints), ], scale = FALSE)"},{"path":"/articles/comparePCA.html","id":"fit-the-dimensionality-reductions-","dir":"Articles","previous_headings":"","what":"Fit the dimensionality reductions.","title":"Comparing DRR and PCA","text":"","code":"t0 <- system.time(pca   <- prcomp(my_data, center = FALSE, scale. = FALSE)) t1 <- system.time(drr.1 <- drr(my_data, verbose = FALSE)) t2 <- system.time(drr.2 <- drr(my_data, fastkrr = 2, verbose = FALSE)) t3 <- system.time(drr.3 <- drr(my_data, fastkrr = 5, verbose = FALSE)) t4 <- system.time(drr.4 <- drr(my_data, fastkrr = 2, fastcv = TRUE,                                verbose = FALSE))"},{"path":[]},{"path":"/articles/comparePCA.html","id":"calculate-rmse","dir":"Articles","previous_headings":"","what":"Calculate RMSE","title":"Comparing DRR and PCA","text":"","code":"rmse <- matrix(NA_real_, nrow = 5, ncol = nvars,                dimnames = list(c(\"pca\", \"drr.1\", \"drr.2\", \"drr.3\", \"drr.4\"),                                seq_len(nvars)))  for (i in seq_len(nvars)){     pca_inv <-         pca$x[, 1:i, drop = FALSE] %*%         t(pca$rotation[, 1:i, drop = FALSE])     rmse[\"pca\",   i] <-         sqrt( sum( (             my_data - pca_inv         ) ^ 2 ) )     rmse[\"drr.1\", i] <-         sqrt( sum( (             my_data - drr.1$inverse(drr.1$fitted.data[, 1:i, drop = FALSE])         ) ^ 2 ) )     rmse[\"drr.2\", i] <-         sqrt( sum( (             my_data - drr.2$inverse(drr.2$fitted.data[, 1:i, drop = FALSE])         ) ^ 2) )     rmse[\"drr.3\", i] <-         sqrt( sum( (             my_data - drr.3$inverse(drr.3$fitted.data[, 1:i, drop = FALSE])         ) ^ 2) )     rmse[\"drr.4\", i] <-         sqrt( sum( (             my_data - drr.4$inverse(drr.4$fitted.data[, 1:i, drop = FALSE])         ) ^ 2) ) }"},{"path":"/articles/comparePCA.html","id":"the-results","dir":"Articles","previous_headings":"","what":"The Results","title":"Comparing DRR and PCA","text":"blocks fastkrr speed calculation, bad accuracy.","code":""},{"path":"/articles/comparePCA.html","id":"rmse","dir":"Articles","previous_headings":"The Results","what":"RMSE","title":"Comparing DRR and PCA","text":"","code":"##              1        2        3            4 ## pca   7.166770 3.899313 1.884524 1.303300e-14 ## drr.1 5.602073 3.436881 1.709814 1.573468e-14 ## drr.2 5.514719 3.211950 1.643146 1.591310e-14 ## drr.3 5.726228 3.496065 1.724555 1.577765e-14 ## drr.4 5.547636 2.889650 1.643971 1.697134e-14"},{"path":"/articles/comparePCA.html","id":"processing-time","dir":"Articles","previous_headings":"The Results","what":"Processing time","title":"Comparing DRR and PCA","text":"","code":"##       user.self sys.self elapsed ## pca       0.002    0.000   0.002 ## drr.1    21.001    0.048  21.093 ## drr.2    12.456    0.016  12.474 ## drr.3    25.813    0.008  25.828 ## drr.4    23.802    0.032  23.838"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Guido Kraemer. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kraemer G (2022). DRR: Dimensionality Reduction via Regression. R package version 0.0.4.9001, https://www.guido-kraemer.com/software/drr/.","code":"@Manual{,   title = {DRR: Dimensionality Reduction via Regression},   author = {Guido Kraemer},   year = {2022},   note = {R package version 0.0.4.9001},   url = {https://www.guido-kraemer.com/software/drr/}, }"},{"path":"/index.html","id":"drr","dir":"","previous_headings":"","what":"Dimensionality Reduction via Regression","title":"Dimensionality Reduction via Regression","text":"Dimensionality Reduction via Regression implementation Dimensionality Reduction via Regression using Kernel Ridge Regression.","code":""},{"path":"/index.html","id":"installing","dir":"","previous_headings":"","what":"Installing:","title":"Dimensionality Reduction via Regression","text":"Install CRAN: Load :","code":"## install.packages(\"devtools\") devtools::install_github(\"gdkrmr/DRR\") install.packages(\"DRR\") library(DRR)"},{"path":"/reference/DRR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction via Regression. — DRR-package","title":"Dimensionality Reduction via Regression. — DRR-package","text":"DRR implements Dimensionality Reduction via Regression using Kernel Ridge Regression. also adds faster implementation Kernel Ridge regression can used CVST package.","code":""},{"path":"/reference/DRR-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dimensionality Reduction via Regression. — DRR-package","text":"Funding provided Department Biogeochemical Integration, Empirical Inference Earth System Group, Max Plack Institute Biogeochemistry, Jena.","code":""},{"path":"/reference/DRR-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Dimensionality Reduction via Regression. — DRR-package","text":"Laparra, V., Malo, J., Camps-Valls, G., 2015. Dimensionality     Reduction via Regression Hyperspectral Imagery. IEEE Journal     Selected Topics Signal Processing 9,     1026-1036. doi:10.1109/JSTSP.2015.2417833 Zhang, Y., Duchi, J.C., Wainwright, M.J., 2013. Divide Conquer     Kernel Ridge Regression: Distributed Algorithm Minimax     Optimal Rates. arXiv:1305.5029 [cs, math, stat].","code":""},{"path":[]},{"path":"/reference/DRR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Dimensionality Reduction via Regression. — DRR-package","text":"Maintainer: Guido Kraemer gkraemer@bgc-jena.mpg.de","code":""},{"path":"/reference/constructFastKRRLearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast implementation for Kernel Ridge Regression. — constructFastKRRLearner","title":"Fast implementation for Kernel Ridge Regression. — constructFastKRRLearner","text":"Constructs learner divide conquer version KRR.","code":""},{"path":"/reference/constructFastKRRLearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast implementation for Kernel Ridge Regression. — constructFastKRRLearner","text":"","code":"constructFastKRRLearner()"},{"path":"/reference/constructFastKRRLearner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast implementation for Kernel Ridge Regression. — constructFastKRRLearner","text":"Returns learner similar constructKRRLearner suitable use CV  fastCV.","code":""},{"path":"/reference/constructFastKRRLearner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast implementation for Kernel Ridge Regression. — constructFastKRRLearner","text":"function used CVST package drop replacement constructKRRLearner. implementation approximates inversion kernel Matrix using divide conquer scheme, lowering computational memory complexity \\(O(n^3)\\) \\(O(n^2)\\) \\(O(n^3/m^2)\\) \\(O(n^2/m^2)\\) respectively, m number blocks used (parameter nblocks). Theoretically safe values \\(m\\) \\(< n^{1/3}\\), practically \\(m\\) may little bit larger. function issue warning, value \\(m\\) large.","code":""},{"path":"/reference/constructFastKRRLearner.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fast implementation for Kernel Ridge Regression. — constructFastKRRLearner","text":"Zhang, Y., Duchi, J.C., Wainwright, M.J., 2013. Divide Conquer     Kernel Ridge Regression: Distributed Algorithm Minimax     Optimal Rates. arXiv:1305.5029 [cs, math, stat].","code":""},{"path":[]},{"path":"/reference/constructFastKRRLearner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast implementation for Kernel Ridge Regression. — constructFastKRRLearner","text":"","code":"ns <- noisySinc(1000) nsTest <- noisySinc(1000)  fast.krr <- constructFastKRRLearner() fast.p <- list(kernel=\"rbfdot\", sigma=100, lambda=.1/getN(ns), nblocks = 4) system.time(fast.m <- fast.krr$learn(ns, fast.p)) #>    user  system elapsed  #>   0.120   0.008   0.291  fast.pred <- fast.krr$predict(fast.m, nsTest) sum((fast.pred - nsTest$y)^2) / getN(nsTest) #> [1] 0.01655082  if (FALSE) { krr <- CVST::constructKRRLearner() p <- list(kernel=\"rbfdot\", sigma=100, lambda=.1/getN(ns)) system.time(m <- krr$learn(ns, p)) pred <- krr$predict(m, nsTest) sum((pred - nsTest$y)^2) / getN(nsTest)  plot(ns, col = '#00000030', pch = 19) lines(sort(nsTest$x), fast.pred[order(nsTest$x)], col = '#00C000', lty = 2) lines(sort(nsTest$x), pred[order(nsTest$x)], col = '#0000C0', lty = 2) legend('topleft', legend = c('fast KRR', 'KRR'),        col = c('#00C000', '#0000C0'), lty = 2) }"},{"path":"/reference/drr.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction via Regression — drr","title":"Dimensionality Reduction via Regression — drr","text":"drr Implements Dimensionality Reduction via Regression using Kernel Ridge Regression.","code":""},{"path":"/reference/drr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction via Regression — drr","text":"","code":"drr(   X,   ndim = ncol(X),   lambda = c(0, 10^(-3:2)),   kernel = \"rbfdot\",   kernel.pars = list(sigma = 10^(-3:4)),   pca = TRUE,   pca.center = TRUE,   pca.scale = FALSE,   fastcv = FALSE,   cv.folds = 5,   fastcv.test = NULL,   fastkrr.nblocks = 4,   verbose = TRUE )"},{"path":"/reference/drr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction via Regression — drr","text":"X input data, matrix. ndim number output dimensions regression functions estimated, see details inversion. lambda penalty term Kernel Ridge Regression. kernel kernel function string, see kernel-class details. kernel.pars list parameters kernel. parameter can vector, crossvalidation choose best combination. pca logical, preprocessing using pca. pca.center logical, center data applying pca. pca.scale logical, scale data applying pca. fastcv TRUE uses fastCV, FALSE uses CV crossvalidation. cv.folds using normal crossvalidation, number folds used. fastcv.test optional separate test data set used fastCV, handed option test fastCV. fastkrr.nblocks number blocks used fast KRR, higher numbers faster compute may introduce numerical inaccurracies, see constructFastKRRLearner details. verbose logical, crossvalidation report back.","code":""},{"path":"/reference/drr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction via Regression — drr","text":"list following items: \"fitted.data\" data reduced dimensions. \"pca.means\" means used center original data. \"pca.scale\" standard deviations used scale original data. \"pca.rotation\" rotation matrix PCA. \"models\" list models used estimate dimension. \"apply\" function fit new data estimated model. \"inverse\" function untransform data.","code":""},{"path":"/reference/drr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dimensionality Reduction via Regression — drr","text":"Parameter combination formed cross-validation used select best combination. Cross-validation uses CV fastCV. Pre-treatment data using PCA scaling made \\(\\alpha = Vx\\).  representation reduced dimensions $$y_i = \\alpha - f_i(\\alpha_1, \\ldots, \\alpha_{-1})$$ final DRR representation : $$r = (\\alpha_1, y_2, y_3, \\ldots,y_d)$$ DRR invertible $$\\alpha_i = y_i + f_i(\\alpha_1,\\alpha_2, \\ldots, alpha_{-1})$$ less dimensions estimated, less inverse functions calculating inverse inaccurate.","code":""},{"path":"/reference/drr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Dimensionality Reduction via Regression — drr","text":"Laparra, V., Malo, J., Camps-Valls, G., 2015. Dimensionality     Reduction via Regression Hyperspectral Imagery. IEEE Journal     Selected Topics Signal Processing 9,     1026-1036. doi:10.1109/JSTSP.2015.2417833","code":""},{"path":"/reference/drr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction via Regression — drr","text":"","code":"tt <- seq(0,4*pi, length.out = 200) helix <- cbind(   x = 3 * cos(tt) + rnorm(length(tt), sd = seq(0.1, 1.4, length.out = length(tt))),   y = 3 * sin(tt) + rnorm(length(tt), sd = seq(0.1, 1.4, length.out = length(tt))),   z = 2 * tt      + rnorm(length(tt), sd = seq(0.1, 1.4, length.out = length(tt))) ) helix <- helix[sample(nrow(helix)),] # shuffling data is important!! system.time( drr.fit  <- drr(helix, ndim = 3, cv.folds = 4,                 lambda = 10^(-2:1),                 kernel.pars = list(sigma = 10^(0:3)),                 fastkrr.nblocks = 2, verbose = TRUE,                 fastcv = FALSE) ) #> 2022-07-20 08:10:16: Constructing Axis 1/3 #> predictors:  PC1 PC2 dependent:  PC3  #> sigma=1 kernel=rbfdot lambda=0.01 nblocks=2 ( 2.132942 ) #> sigma=10 kernel=rbfdot lambda=0.01 nblocks=2 ( 3.353147 ) #> sigma=100 kernel=rbfdot lambda=0.01 nblocks=2 ( 4.065498 ) #> sigma=1000 kernel=rbfdot lambda=0.01 nblocks=2 ( 4.110372 ) #> sigma=1 kernel=rbfdot lambda=0.1 nblocks=2 ( 1.867409 ) #> sigma=10 kernel=rbfdot lambda=0.1 nblocks=2 ( 3.395369 ) #> sigma=100 kernel=rbfdot lambda=0.1 nblocks=2 ( 4.068624 ) #> sigma=1000 kernel=rbfdot lambda=0.1 nblocks=2 ( 4.110376 ) #> sigma=1 kernel=rbfdot lambda=1 nblocks=2 ( 2.187618 ) #> sigma=10 kernel=rbfdot lambda=1 nblocks=2 ( 3.654144 ) #> sigma=100 kernel=rbfdot lambda=1 nblocks=2 ( 4.086557 ) #> sigma=1000 kernel=rbfdot lambda=1 nblocks=2 ( 4.110392 ) #> sigma=1 kernel=rbfdot lambda=10 nblocks=2 ( 3.435971 ) #> sigma=10 kernel=rbfdot lambda=10 nblocks=2 ( 4.011086 ) #> sigma=100 kernel=rbfdot lambda=10 nblocks=2 ( 4.105904 ) #> sigma=1000 kernel=rbfdot lambda=10 nblocks=2 ( 4.110409 ) #> 2022-07-20 08:10:17: Constructing Axis 2/3 #> predictors:  PC1 dependent:  PC2  #> sigma=1 kernel=rbfdot lambda=0.01 nblocks=2 ( 2.145742 ) #> sigma=10 kernel=rbfdot lambda=0.01 nblocks=2 ( 2.803929 ) #> sigma=100 kernel=rbfdot lambda=0.01 nblocks=2 ( 3.667547 ) #> sigma=1000 kernel=rbfdot lambda=0.01 nblocks=2 ( 4.593707 ) #> sigma=1 kernel=rbfdot lambda=0.1 nblocks=2 ( 1.714385 ) #> sigma=10 kernel=rbfdot lambda=0.1 nblocks=2 ( 2.416549 ) #> sigma=100 kernel=rbfdot lambda=0.1 nblocks=2 ( 3.669684 ) #> sigma=1000 kernel=rbfdot lambda=0.1 nblocks=2 ( 4.60251 ) #> sigma=1 kernel=rbfdot lambda=1 nblocks=2 ( 1.802629 ) #> sigma=10 kernel=rbfdot lambda=1 nblocks=2 ( 2.922534 ) #> sigma=100 kernel=rbfdot lambda=1 nblocks=2 ( 4.183214 ) #> sigma=1000 kernel=rbfdot lambda=1 nblocks=2 ( 4.837981 ) #> sigma=1 kernel=rbfdot lambda=10 nblocks=2 ( 3.424796 ) #> sigma=10 kernel=rbfdot lambda=10 nblocks=2 ( 4.481173 ) #> sigma=100 kernel=rbfdot lambda=10 nblocks=2 ( 4.993442 ) #> sigma=1000 kernel=rbfdot lambda=10 nblocks=2 ( 5.152819 ) #> 2022-07-20 08:10:18: Constructing Axis 3/3 #>    user  system elapsed  #>   1.992   0.000   2.012   if (FALSE) { library(rgl) plot3d(helix) points3d(drr.fit$inverse(drr.fit$fitted.data[,1,drop = FALSE]), col = 'blue') points3d(drr.fit$inverse(drr.fit$fitted.data[,1:2]),             col = 'red')  plot3d(drr.fit$fitted.data) pad <- -3 fd <- drr.fit$fitted.data xx <- seq(min(fd[,1]),       max(fd[,1]),       length.out = 25) yy <- seq(min(fd[,2]) - pad, max(fd[,2]) + pad, length.out = 5) zz <- seq(min(fd[,3]) - pad, max(fd[,3]) + pad, length.out = 5)  dd <- as.matrix(expand.grid(xx, yy, zz)) plot3d(helix) for(y in yy) for(x in xx)   rgl.linestrips(drr.fit$inverse(cbind(x, y, zz)), col = 'blue') for(y in yy) for(z in zz)   rgl.linestrips(drr.fit$inverse(cbind(xx, y, z)), col = 'blue') for(x in xx) for(z in zz)   rgl.linestrips(drr.fit$inverse(cbind(x, yy, z)), col = 'blue') }"}]
